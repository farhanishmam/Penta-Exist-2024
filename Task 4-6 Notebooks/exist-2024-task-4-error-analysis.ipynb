{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Preamble: Install and Import Packages"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T18:15:16.598715Z","iopub.status.busy":"2024-05-06T18:15:16.597882Z","iopub.status.idle":"2024-05-06T18:15:25.498522Z","shell.execute_reply":"2024-05-06T18:15:25.497700Z","shell.execute_reply.started":"2024-05-06T18:15:16.598676Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\LaptopAid\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision.transforms import Resize\n","from torchvision.io import read_image, ImageReadMode\n","from multilingual_clip import Config_MCLIP\n","import open_clip\n","import json\n","import pandas as pd\n","import random\n","from pathlib import Path\n","import numpy as np\n","import transformers as hf\n","from tqdm.auto import tqdm\n","from sklearn.metrics import f1_score\n","from PIL import Image\n","import os\n","import time\n","import math\n","import matplotlib.pyplot as plt\n","import seaborn as sns"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T18:15:44.872551Z","iopub.status.busy":"2024-05-06T18:15:44.871975Z","iopub.status.idle":"2024-05-06T18:15:44.877783Z","shell.execute_reply":"2024-05-06T18:15:44.876660Z","shell.execute_reply.started":"2024-05-06T18:15:44.872513Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["4.39.2\n"]},{"data":{"text/plain":["<torch.autograd.anomaly_mode.set_detect_anomaly at 0x21a599329f0>"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["print(hf.__version__)\n","torch.autograd.set_detect_anomaly(True)"]},{"cell_type":"markdown","metadata":{},"source":["# Initialise the Configuration and Random Seeds"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T18:18:38.260806Z","iopub.status.busy":"2024-05-06T18:18:38.259845Z","iopub.status.idle":"2024-05-06T18:18:38.990037Z","shell.execute_reply":"2024-05-06T18:18:38.989149Z","shell.execute_reply.started":"2024-05-06T18:18:38.260764Z"},"trusted":true},"outputs":[],"source":["_text_model_config = {}\n","\n","_image_model_config = {\n","    \"attention_probs_dropout_prob\": 0.0,\n","    \"encoder_stride\": 16,\n","    \"hidden_act\": \"gelu\",\n","    \"hidden_dropout_prob\": 0.0,\n","    \"hidden_size\": 768,\n","    \"image_size\": 224,\n","    \"initializer_range\": 0.02,\n","    \"intermediate_size\": 3072,\n","    \"layer_norm_eps\": 1e-12,\n","    \"num_attention_heads\": 12,\n","    \"num_channels\": 3,\n","    \"num_hidden_layers\": 0,\n","    \"patch_size\": 16,\n","    \"qkv_bias\": True,\n","}\n","\n","# Dual encoder/Concat\n","tokeniser_model_id = 'xlm-roberta-base'\n","text_model_id = 'xlm-roberta-base'\n","image_model_id = 'google/vit-base-patch16-224-in21k'\n","\n","# CLIP\n","# multimodal_model_id = 'openai/clip-vit-base-patch32'\n","\n","# M-CLIP\n","# tokeniser_model_id = 'M-CLIP/XLM-Roberta-Large-Vit-B-16Plus'\n","# text_model_id = 'M-CLIP/XLM-Roberta-Large-Vit-B-16Plus'\n","# image_model_id = 'ViT-B-16-plus-240'\n","image_training_id = 'laion400m_e32'\n","\n","# ViLT\n","multimodal_model_id = 'dandelin/vilt-b32-mlm'\n","\n","\n","class CFG:\n","    use_multimodal = True\n","    use_dualencoder = False\n","    split_lang = False\n","    save_models = False\n","    use_lstm = False\n","    use_attn = False\n","    use_mask_split = False\n","    use_modal_attn = True\n","    is_mclip = False\n","    init_weights = False\n","    tokeniser_model_id = tokeniser_model_id\n","    text_model_id = text_model_id\n","    image_model_id = image_model_id\n","    multimodal_model_id = multimodal_model_id\n","    image_training_id = image_training_id\n","    text_model_config = hf.AutoConfig.from_pretrained(text_model_id) if not 'M-CLIP' in text_model_id else None\n","    image_model_config = hf.AutoConfig.from_pretrained(image_model_id) if not 'M-CLIP' in text_model_id else None\n","    multimodal_model_config = hf.AutoConfig.from_pretrained(multimodal_model_id, text_config=_text_model_config, vision_config=_image_model_config)\n","    images_base_path = Path(f'EXIST 2024 Lab/EXIST 2024 Memes Dataset/training/memes')\n","    images_base_path_test = Path('EXIST 2024 Lab/EXIST 2024 Memes Dataset/test/memes')\n","    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","    debug = True\n","    print_freq = 300\n","    apex = True # for faster training\n","    epochs = 10\n","    learning_rate = 2e-4  # for adam optimizer\n","    eps = 1e-6\n","    betas = (0.9, 0.999)  # for adam optimizer\n","    batch_size = 64\n","    max_len = 512\n","    weight_decay = 0.01  # for adam optimizer regulaization parameter\n","    gradient_accumulation_steps = 1\n","    max_grad_norm = 1000\n","    seed = 42\n","    train = True\n","    num_class = 2  # Number of class in your dataset\n","    mlp_hidden_size = 256\n","    mlp_hidden_layers = 0\n","    mlp_dropout = 0.1\n","    mlp_grad_clip = 1.0\n","    mlp_init_range = 0.2\n","    mlp_attn_dim = 256"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T18:16:08.601112Z","iopub.status.busy":"2024-05-06T18:16:08.600732Z","iopub.status.idle":"2024-05-06T18:16:08.610179Z","shell.execute_reply":"2024-05-06T18:16:08.609275Z","shell.execute_reply.started":"2024-05-06T18:16:08.601082Z"},"trusted":true},"outputs":[],"source":["def seed_everything(seed=42):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    \n","seed_everything(CFG.seed)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T18:16:13.511690Z","iopub.status.busy":"2024-05-06T18:16:13.511328Z","iopub.status.idle":"2024-05-06T18:16:13.542982Z","shell.execute_reply":"2024-05-06T18:16:13.542212Z","shell.execute_reply.started":"2024-05-06T18:16:13.511661Z"},"trusted":true},"outputs":[],"source":["class MultilingualCLIP(hf.PreTrainedModel):\n","    config_class = Config_MCLIP.MCLIPConfig\n","\n","    def __init__(self, config, *args, **kwargs):\n","        super().__init__(config, *args, **kwargs)\n","        self.transformer = hf.AutoModel.from_pretrained(config.modelBase, cache_dir=kwargs.get(\"cache_dir\"))\n","        self.LinearTransformation = torch.nn.Linear(in_features=config.transformerDimensions,\n","                                                    out_features=config.numDims)\n","\n","    def forward(self, tokens, mask):\n","        embs = self.transformer(tokens, attention_mask=mask)[0]\n","        embs = (embs * mask.unsqueeze(2)).sum(dim=1) / mask.sum(dim=1)[:, None]\n","        return self.LinearTransformation(embs)\n","\n","    @classmethod\n","    def _load_state_dict_into_model(cls, model, state_dict, pretrained_model_name_or_path, _fast_init=True):\n","        model.load_state_dict(state_dict)\n","        return model, [], [], []"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["task4_ea = ['211702', '211424', '110981', '110664']"]},{"cell_type":"markdown","metadata":{},"source":["# Preprocess the Dataset"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T18:18:08.228459Z","iopub.status.busy":"2024-05-06T18:18:08.227674Z","iopub.status.idle":"2024-05-06T18:18:08.559596Z","shell.execute_reply":"2024-05-06T18:18:08.558616Z","shell.execute_reply.started":"2024-05-06T18:18:08.228415Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(4, 16)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id_EXIST</th>\n","      <th>lang</th>\n","      <th>text</th>\n","      <th>meme</th>\n","      <th>path_memes</th>\n","      <th>number_annotators</th>\n","      <th>annotators</th>\n","      <th>gender_annotators</th>\n","      <th>age_annotators</th>\n","      <th>ethnicities_annotators</th>\n","      <th>study_levels_annotators</th>\n","      <th>countries_annotators</th>\n","      <th>labels_task4</th>\n","      <th>labels_task5</th>\n","      <th>labels_task6</th>\n","      <th>split</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>110664</th>\n","      <td>110664</td>\n","      <td>es</td>\n","      <td>EL FISICO ATRAE PERO LA ENFERMERA ENAMORA</td>\n","      <td>110664.jpeg</td>\n","      <td>memes/110664.jpeg</td>\n","      <td>6</td>\n","      <td>[Annotator_145, Annotator_146, Annotator_147, ...</td>\n","      <td>[F, F, F, M, M, M]</td>\n","      <td>[18-22, 23-45, 46+, 46+, 18-22, 23-45]</td>\n","      <td>[White or Caucasian, White or Caucasian, White...</td>\n","      <td>[High school degree or equivalent, Bachelorâ€™s ...</td>\n","      <td>[Spain, Italy, Portugal, Spain, Portugal, Mexico]</td>\n","      <td>[YES, YES, YES, YES, YES, YES]</td>\n","      <td>[DIRECT, DIRECT, DIRECT, DIRECT, DIRECT, UNKNOWN]</td>\n","      <td>[[SEXUAL-VIOLENCE], [STEREOTYPING-DOMINANCE, O...</td>\n","      <td>TRAIN-MEME_ES</td>\n","    </tr>\n","    <tr>\n","      <th>110981</th>\n","      <td>110981</td>\n","      <td>es</td>\n","      <td>QUIERE IGUALDAD DE OPORTUNIDADES EN LA EDUCACI...</td>\n","      <td>110981.jpeg</td>\n","      <td>memes/110981.jpeg</td>\n","      <td>6</td>\n","      <td>[Annotator_217, Annotator_218, Annotator_219, ...</td>\n","      <td>[F, F, F, M, M, M]</td>\n","      <td>[18-22, 23-45, 46+, 46+, 18-22, 23-45]</td>\n","      <td>[Hispano or Latino, White or Caucasian, White ...</td>\n","      <td>[Bachelorâ€™s degree, Masterâ€™s degree, Bachelorâ€™...</td>\n","      <td>[Peru, Portugal, Portugal, Chile, Germany, Spain]</td>\n","      <td>[NO, NO, NO, NO, NO, NO]</td>\n","      <td>[-, -, -, -, -, -]</td>\n","      <td>[[-], [-], [-], [-], [-], [-]]</td>\n","      <td>TRAIN-MEME_ES</td>\n","    </tr>\n","    <tr>\n","      <th>211424</th>\n","      <td>211424</td>\n","      <td>en</td>\n","      <td>SEXUAL ORIENTATION? ORIENTED TO WHEREVER THE H...</td>\n","      <td>211424.jpeg</td>\n","      <td>memes/211424.jpeg</td>\n","      <td>6</td>\n","      <td>[Annotator_752, Annotator_753, Annotator_754, ...</td>\n","      <td>[F, F, F, M, M, M]</td>\n","      <td>[18-22, 23-45, 46+, 18-22, 23-45, 46+]</td>\n","      <td>[White or Caucasian, White or Caucasian, White...</td>\n","      <td>[Masterâ€™s degree, High school degree or equiva...</td>\n","      <td>[Portugal, Slovenia, Australia, Germany, Austr...</td>\n","      <td>[YES, YES, YES, YES, YES, YES]</td>\n","      <td>[DIRECT, DIRECT, DIRECT, DIRECT, DIRECT, DIRECT]</td>\n","      <td>[[OBJECTIFICATION, SEXUAL-VIOLENCE], [OBJECTIF...</td>\n","      <td>TRAIN-MEME_EN</td>\n","    </tr>\n","    <tr>\n","      <th>211702</th>\n","      <td>211702</td>\n","      <td>en</td>\n","      <td>me laying in bed at night thinking about how m...</td>\n","      <td>211702.jpeg</td>\n","      <td>memes/211702.jpeg</td>\n","      <td>6</td>\n","      <td>[Annotator_817, Annotator_818, Annotator_819, ...</td>\n","      <td>[F, F, F, M, M, M]</td>\n","      <td>[18-22, 23-45, 46+, 18-22, 23-45, 46+]</td>\n","      <td>[Asian, White or Caucasian, White or Caucasian...</td>\n","      <td>[High school degree or equivalent, Masterâ€™s de...</td>\n","      <td>[China, Hungary, United States, United Kingdom...</td>\n","      <td>[NO, NO, NO, NO, NO, NO]</td>\n","      <td>[-, -, -, -, -, -]</td>\n","      <td>[[-], [-], [-], [-], [-], [-]]</td>\n","      <td>TRAIN-MEME_EN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       id_EXIST lang                                               text  \\\n","110664   110664   es         EL FISICO ATRAE PERO LA ENFERMERA ENAMORA    \n","110981   110981   es  QUIERE IGUALDAD DE OPORTUNIDADES EN LA EDUCACI...   \n","211424   211424   en  SEXUAL ORIENTATION? ORIENTED TO WHEREVER THE H...   \n","211702   211702   en  me laying in bed at night thinking about how m...   \n","\n","               meme         path_memes number_annotators  \\\n","110664  110664.jpeg  memes/110664.jpeg                 6   \n","110981  110981.jpeg  memes/110981.jpeg                 6   \n","211424  211424.jpeg  memes/211424.jpeg                 6   \n","211702  211702.jpeg  memes/211702.jpeg                 6   \n","\n","                                               annotators   gender_annotators  \\\n","110664  [Annotator_145, Annotator_146, Annotator_147, ...  [F, F, F, M, M, M]   \n","110981  [Annotator_217, Annotator_218, Annotator_219, ...  [F, F, F, M, M, M]   \n","211424  [Annotator_752, Annotator_753, Annotator_754, ...  [F, F, F, M, M, M]   \n","211702  [Annotator_817, Annotator_818, Annotator_819, ...  [F, F, F, M, M, M]   \n","\n","                                age_annotators  \\\n","110664  [18-22, 23-45, 46+, 46+, 18-22, 23-45]   \n","110981  [18-22, 23-45, 46+, 46+, 18-22, 23-45]   \n","211424  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n","211702  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n","\n","                                   ethnicities_annotators  \\\n","110664  [White or Caucasian, White or Caucasian, White...   \n","110981  [Hispano or Latino, White or Caucasian, White ...   \n","211424  [White or Caucasian, White or Caucasian, White...   \n","211702  [Asian, White or Caucasian, White or Caucasian...   \n","\n","                                  study_levels_annotators  \\\n","110664  [High school degree or equivalent, Bachelorâ€™s ...   \n","110981  [Bachelorâ€™s degree, Masterâ€™s degree, Bachelorâ€™...   \n","211424  [Masterâ€™s degree, High school degree or equiva...   \n","211702  [High school degree or equivalent, Masterâ€™s de...   \n","\n","                                     countries_annotators  \\\n","110664  [Spain, Italy, Portugal, Spain, Portugal, Mexico]   \n","110981  [Peru, Portugal, Portugal, Chile, Germany, Spain]   \n","211424  [Portugal, Slovenia, Australia, Germany, Austr...   \n","211702  [China, Hungary, United States, United Kingdom...   \n","\n","                          labels_task4  \\\n","110664  [YES, YES, YES, YES, YES, YES]   \n","110981        [NO, NO, NO, NO, NO, NO]   \n","211424  [YES, YES, YES, YES, YES, YES]   \n","211702        [NO, NO, NO, NO, NO, NO]   \n","\n","                                             labels_task5  \\\n","110664  [DIRECT, DIRECT, DIRECT, DIRECT, DIRECT, UNKNOWN]   \n","110981                                 [-, -, -, -, -, -]   \n","211424   [DIRECT, DIRECT, DIRECT, DIRECT, DIRECT, DIRECT]   \n","211702                                 [-, -, -, -, -, -]   \n","\n","                                             labels_task6          split  \n","110664  [[SEXUAL-VIOLENCE], [STEREOTYPING-DOMINANCE, O...  TRAIN-MEME_ES  \n","110981                     [[-], [-], [-], [-], [-], [-]]  TRAIN-MEME_ES  \n","211424  [[OBJECTIFICATION, SEXUAL-VIOLENCE], [OBJECTIF...  TRAIN-MEME_EN  \n","211702                     [[-], [-], [-], [-], [-], [-]]  TRAIN-MEME_EN  "]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["with open('EXIST 2024 Lab/EXIST 2024 Memes Dataset/training/EXIST2024_training.json', 'r', encoding='utf-8') as fp:\n","    annotations = json.load(fp)\n","df = pd.DataFrame.from_dict(annotations).T\n","df = df[df['id_EXIST'].isin(task4_ea)]\n","print(df.shape)\n","df.head()"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T18:18:08.962373Z","iopub.status.busy":"2024-05-06T18:18:08.962006Z","iopub.status.idle":"2024-05-06T18:18:08.981050Z","shell.execute_reply":"2024-05-06T18:18:08.980036Z","shell.execute_reply.started":"2024-05-06T18:18:08.962344Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id_EXIST</th>\n","      <th>meme</th>\n","      <th>text</th>\n","      <th>lang</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>110664</td>\n","      <td>110664.jpeg</td>\n","      <td>EL FISICO ATRAE PERO LA ENFERMERA ENAMORA</td>\n","      <td>es</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>110981</td>\n","      <td>110981.jpeg</td>\n","      <td>QUIERE IGUALDAD DE OPORTUNIDADES EN LA EDUCACI...</td>\n","      <td>es</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>211424</td>\n","      <td>211424.jpeg</td>\n","      <td>SEXUAL ORIENTATION? ORIENTED TO WHEREVER THE H...</td>\n","      <td>en</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>211702</td>\n","      <td>211702.jpeg</td>\n","      <td>me laying in bed at night thinking about how m...</td>\n","      <td>en</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id_EXIST         meme                                               text  \\\n","0    110664  110664.jpeg         EL FISICO ATRAE PERO LA ENFERMERA ENAMORA    \n","1    110981  110981.jpeg  QUIERE IGUALDAD DE OPORTUNIDADES EN LA EDUCACI...   \n","2    211424  211424.jpeg  SEXUAL ORIENTATION? ORIENTED TO WHEREVER THE H...   \n","3    211702  211702.jpeg  me laying in bed at night thinking about how m...   \n","\n","  lang  \n","0   es  \n","1   es  \n","2   en  \n","3   en  "]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["mini_df = df[['id_EXIST', 'meme', 'text', 'lang']].reset_index(drop=True)\n","mini_df['id_EXIST'] = pd.to_numeric(mini_df['id_EXIST'])\n","mini_df.head()"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T18:18:09.679379Z","iopub.status.busy":"2024-05-06T18:18:09.678690Z","iopub.status.idle":"2024-05-06T18:18:09.740465Z","shell.execute_reply":"2024-05-06T18:18:09.739514Z","shell.execute_reply.started":"2024-05-06T18:18:09.679345Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["4\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id_EXIST</th>\n","      <th>meme</th>\n","      <th>text</th>\n","      <th>lang</th>\n","      <th>label_task4</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>110664</td>\n","      <td>110664.jpeg</td>\n","      <td>EL FISICO ATRAE PERO LA ENFERMERA ENAMORA</td>\n","      <td>es</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>110981</td>\n","      <td>110981.jpeg</td>\n","      <td>QUIERE IGUALDAD DE OPORTUNIDADES EN LA EDUCACI...</td>\n","      <td>es</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>211424</td>\n","      <td>211424.jpeg</td>\n","      <td>SEXUAL ORIENTATION? ORIENTED TO WHEREVER THE H...</td>\n","      <td>en</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>211702</td>\n","      <td>211702.jpeg</td>\n","      <td>me laying in bed at night thinking about how m...</td>\n","      <td>en</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id_EXIST         meme                                               text  \\\n","0    110664  110664.jpeg         EL FISICO ATRAE PERO LA ENFERMERA ENAMORA    \n","1    110981  110981.jpeg  QUIERE IGUALDAD DE OPORTUNIDADES EN LA EDUCACI...   \n","2    211424  211424.jpeg  SEXUAL ORIENTATION? ORIENTED TO WHEREVER THE H...   \n","3    211702  211702.jpeg  me laying in bed at night thinking about how m...   \n","\n","  lang  label_task4  \n","0   es            1  \n","1   es            0  \n","2   en            1  \n","3   en            0  "]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["task4_gold_path = Path('EXIST 2024 Lab/evaluation/golds/EXIST2024_training_task4_gold_hard.json')\n","task5_gold_path = Path('EXIST 2024 Lab/evaluation/golds/EXIST2024_training_task5_gold_hard.json')\n","task6_gold_path = Path('EXIST 2024 Lab/evaluation/golds/EXIST2024_training_task6_gold_hard.json')\n","task4_gold = pd.read_json(task4_gold_path)\n","\n","choices = ['YES', 'NO']\n","mini_df = pd.merge(mini_df, task4_gold, left_on='id_EXIST', right_on='id', how='left').drop(columns=['id', 'test_case']).rename(columns={'value': 'label_task4'})\n","mini_df['label_task4'] = mini_df['label_task4'].apply(lambda x: np.random.choice(choices) if pd.isna(x) else x)\n","mini_df['label_task4'] = pd.to_numeric(mini_df['label_task4'].map({'YES': 1, 'NO': 0}))\n","print(len(mini_df))\n","mini_df.head()"]},{"cell_type":"markdown","metadata":{},"source":["# Initialise the Processors/Tokenisers/Models"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T18:18:44.737506Z","iopub.status.busy":"2024-05-06T18:18:44.737103Z","iopub.status.idle":"2024-05-06T18:19:08.560431Z","shell.execute_reply":"2024-05-06T18:19:08.559371Z","shell.execute_reply.started":"2024-05-06T18:18:44.737475Z"},"trusted":true},"outputs":[],"source":["if CFG.is_mclip:\n","    tokenizer = hf.AutoTokenizer.from_pretrained(CFG.tokeniser_model_id)\n","    text_model = MultilingualCLIP.from_pretrained(CFG.text_model_id).to(CFG.device)\n","    image_model, _, image_processor = open_clip.create_model_and_transforms(CFG.image_model_id, pretrained=CFG.image_training_id)\n","    image_model = image_model.to(CFG.device)\n","elif CFG.use_multimodal:\n","    mm_processor = hf.AutoProcessor.from_pretrained(CFG.multimodal_model_id)\n","    mm_model = hf.AutoModel.from_pretrained(CFG.multimodal_model_id).to(CFG.device)\n","elif CFG.use_dualencoder:\n","    tokenizer = hf.AutoTokenizer.from_pretrained(CFG.tokeniser_model_id, padding=True, truncation=True)\n","    processor = hf.AutoImageProcessor.from_pretrained(CFG.image_model_id)\n","    de_processor = hf.VisionTextDualEncoderProcessor(image_processor=processor, tokenizer=tokenizer)\n","    text_model = hf.AutoModel.from_pretrained(CFG.text_model_id).to(CFG.device)\n","    image_model = hf.AutoModel.from_pretrained(CFG.image_model_id).to(CFG.device)\n","    de_model = hf.VisionTextDualEncoderModel(vision_model=image_model, text_model=text_model)\n","else:\n","    tokenizer = hf.AutoTokenizer.from_pretrained(CFG.tokeniser_model_id)\n","    text_model = hf.AutoModel.from_pretrained(CFG.text_model_id).to(CFG.device)\n","    # Adding a config to the image_model gets rid of lots of pretrained weights\n","    image_model = hf.AutoModel.from_pretrained(CFG.image_model_id).to(CFG.device)"]},{"cell_type":"markdown","metadata":{},"source":["# Custom Dataset Definition"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T18:19:14.449356Z","iopub.status.busy":"2024-05-06T18:19:14.448759Z","iopub.status.idle":"2024-05-06T18:19:14.463944Z","shell.execute_reply":"2024-05-06T18:19:14.462848Z","shell.execute_reply.started":"2024-05-06T18:19:14.449326Z"},"trusted":true},"outputs":[],"source":["class ExistDataset(Dataset):\n","    def __init__(self, features, img_dir, labels=None, test=False, img_transform=None, caption_transform=None, target_transform=None):\n","        self.features = features\n","        self.labels = labels\n","        self.img_dir = img_dir\n","        self.test = test\n","        self.img_transform = img_transform\n","        self.caption_transform = caption_transform\n","        self.target_transform = target_transform\n","\n","    def __len__(self):\n","        return len(self.features)\n","\n","    def __getitem__(self, idx):\n","        img_path = str(self.img_dir.joinpath(self.features['meme'].iloc[idx]))\n","        if CFG.is_mclip:\n","            image = Image.open(img_path)\n","        else:\n","            image = read_image(img_path, mode=ImageReadMode.RGB).to(device=CFG.device)\n","        caption = self.features['text'].iloc[idx]\n","        \n","        if not self.test:\n","            label = self.labels.iloc[idx]\n","        else:\n","            identity = self.features['id_EXIST'].iloc[idx]\n","        \n","        if self.img_transform:\n","            image = self.img_transform(image)\n","        if self.caption_transform:\n","            caption = self.caption_transform(caption)\n","        if not self.test and self.target_transform:\n","            label = self.target_transform(label)\n","            \n","        if CFG.split_lang:\n","            caption = f'Language: {self.features[\"lang\"].iloc[idx]} - {caption}'\n","            \n","        if CFG.is_mclip:\n","            processed = tokenizer(caption, padding=True, return_tensors='pt')\n","            seq = processed['input_ids']\n","            mask = processed['attention_mask']\n","            image = image_processor(image)\n","        elif CFG.use_multimodal:\n","            processed = mm_processor(text=caption, images=image, return_tensors=\"pt\", padding=True, truncation=True)\n","            seq = processed['input_ids']\n","            mask = processed['attention_mask']\n","            image = processed['pixel_values']\n","        elif CFG.use_dualencoder:\n","            processed = de_processor(text=caption, images=image, return_tensors=\"pt\")\n","            seq = processed['input_ids']\n","            mask = processed['attention_mask']\n","            image = processed['pixel_values']\n","        else:\n","            processed = tokenizer.encode_plus(\n","                caption,\n","                padding='longest',\n","                truncation=True,\n","                return_tensors='pt'\n","            )\n","            seq = processed['input_ids']\n","            mask = processed['attention_mask']\n","        \n","        if not self.test:\n","            label = torch.tensor([label]).long()\n","            return image, seq, mask, label\n","        \n","        return identity, image, seq, mask"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T18:19:14.912249Z","iopub.status.busy":"2024-05-06T18:19:14.911773Z","iopub.status.idle":"2024-05-06T18:19:14.920983Z","shell.execute_reply":"2024-05-06T18:19:14.919959Z","shell.execute_reply.started":"2024-05-06T18:19:14.912215Z"},"trusted":true},"outputs":[],"source":["class Collator(object):\n","    def __init__(self, test=False):\n","        self.test = test\n","    def __call__(self, batch):\n","        if not self.test:\n","            images, seqs, masks, labels = zip(*batch)\n","            labels = torch.stack(labels)\n","        else:\n","            ids, images, seqs, masks = zip(*batch)\n","\n","        seqs = [seq.squeeze(dim=0) for seq in seqs]\n","        masks = [mask.squeeze(dim=0) for mask in masks]\n","        images = [image.squeeze(dim=0) for image in images]\n","\n","        seqs = nn.utils.rnn.pad_sequence(seqs, batch_first=True)\n","        masks = nn.utils.rnn.pad_sequence(masks, batch_first=True)\n","\n","        images = torch.stack(images)\n","        \n","        if not self.test:\n","            return images, seqs, masks, labels\n","        \n","        return ids, images, seqs, masks"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T18:19:15.535724Z","iopub.status.busy":"2024-05-06T18:19:15.535132Z","iopub.status.idle":"2024-05-06T18:19:15.540414Z","shell.execute_reply":"2024-05-06T18:19:15.539547Z","shell.execute_reply.started":"2024-05-06T18:19:15.535690Z"},"trusted":true},"outputs":[],"source":["resizer = Resize((224, 224), antialias=True)\n","\n","def resize_images(img_tensor):\n","    return resizer(img_tensor)"]},{"cell_type":"markdown","metadata":{},"source":["# Dataset Initialisation"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T18:19:21.839823Z","iopub.status.busy":"2024-05-06T18:19:21.839454Z","iopub.status.idle":"2024-05-06T18:19:21.846397Z","shell.execute_reply":"2024-05-06T18:19:21.845390Z","shell.execute_reply.started":"2024-05-06T18:19:21.839793Z"},"trusted":true},"outputs":[{"data":{"text/plain":["4"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["val_dataset = ExistDataset(mini_df, CFG.images_base_path, labels=mini_df['label_task4'], img_transform=resize_images, test=True)\n","len(val_dataset)"]},{"cell_type":"markdown","metadata":{},"source":["# Model Architecture"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T18:19:25.859218Z","iopub.status.busy":"2024-05-06T18:19:25.858470Z","iopub.status.idle":"2024-05-06T18:19:25.882149Z","shell.execute_reply":"2024-05-06T18:19:25.881167Z","shell.execute_reply.started":"2024-05-06T18:19:25.859188Z"},"trusted":true},"outputs":[],"source":["class ConcatArch(nn.Module):\n","    def __init__(self, hidden_size, hidden_layers, dropout, num_classes, use_multimodal=False, use_dualencoder=False, is_mclip=False):\n","        super().__init__()\n","        \n","        self.hidden_size = hidden_size\n","        self.hidden_layers = hidden_layers\n","        self.use_multimodal = use_multimodal\n","        self.use_dualencoder = use_dualencoder\n","        self.is_mclip = is_mclip\n","        self.is_vilt = 'ViltForMaskedLM' in CFG.multimodal_model_config.architectures\n","        \n","        if self.is_mclip:\n","            self.text_model = text_model\n","            self.image_model = image_model\n","        elif self.use_multimodal:\n","            self.mm_model = mm_model\n","        elif self.use_dualencoder:\n","            self.de_model = de_model\n","        else:\n","            self.text_model = text_model\n","            self.image_model = image_model\n","        \n","        if self.is_mclip:\n","            self.fc1 = nn.Linear(1280, self.hidden_size)\n","        elif self.use_multimodal:\n","            if self.is_vilt and CFG.use_lstm:\n","                out_channels = CFG.mlp_hidden_size + CFG.multimodal_model_config.hidden_size\n","                self.lstm = nn.LSTM(CFG.multimodal_model_config.hidden_size, CFG.mlp_hidden_size, batch_first=True)\n","            elif self.is_vilt and CFG.use_mask_split:\n","                out_channels = CFG.multimodal_model_config.hidden_size * 3\n","            elif self.is_vilt and CFG.use_attn:\n","                self.attn = nn.Sequential(\n","                    nn.Linear(CFG.multimodal_model_config.hidden_size, CFG.mlp_attn_dim),\n","                    nn.Tanh(),\n","                    nn.Linear(CFG.mlp_attn_dim, 1),\n","                    nn.Softmax(dim=1)\n","                )\n","            elif self.is_vilt and CFG.use_modal_attn:\n","                self.attn1 = nn.Sequential(\n","                    nn.Linear(CFG.multimodal_model_config.hidden_size, CFG.mlp_attn_dim),\n","                    nn.Tanh(),\n","                    nn.Linear(CFG.mlp_attn_dim, 1),\n","                    nn.Softmax(dim=1)\n","                )\n","                self.attn2 = nn.Sequential(\n","                    nn.Linear(CFG.multimodal_model_config.hidden_size, CFG.mlp_attn_dim),\n","                    nn.Tanh(),\n","                    nn.Linear(CFG.mlp_attn_dim, 1),\n","                    nn.Softmax(dim=1)\n","                )\n","                out_channels = CFG.multimodal_model_config.hidden_size * 2\n","            elif self.is_vilt:\n","                out_channels = CFG.multimodal_model_config.hidden_size\n","            else:\n","                out_channels = 2 * CFG.multimodal_model_config.projection_dim\n","            self.fc1 = nn.Linear(out_channels, self.hidden_size)\n","        elif self.use_dualencoder:\n","            self.fc1 = nn.Linear(2 * 512, self.hidden_size)\n","        else:\n","            self.fc1 = nn.Linear(CFG.text_model_config.hidden_size + CFG.image_model_config.hidden_size, self.hidden_size)\n","        self.hiddens = nn.ModuleList([nn.Linear(self.hidden_size, self.hidden_size) for _ in range(self.hidden_layers)])\n","        self.fc2 = nn.Linear(self.hidden_size, num_classes)\n","        self.activation = nn.ReLU()\n","        self.dropout = nn.Dropout(dropout)\n","        \n","        if CFG.init_weights:\n","            self._init_weights(self.fc1)\n","            for hidden in self.hiddens:\n","                self._init_weights(hidden)\n","            self._init_weights(self.fc2)\n","\n","    def forward(self, tokens, mask, image):\n","        text_attentions = None\n","        img_attentions = None\n","        \n","        if self.is_mclip:\n","            emb_text = self.text_model.forward(tokens, mask)\n","            emb_img = self.image_model.encode_image(image)\n","            x = torch.cat([emb_text, emb_img], dim=1)\n","        elif self.use_multimodal:\n","            mm_output = self.mm_model(input_ids=tokens, attention_mask=mask, pixel_values=image, output_hidden_states=True)\n","            cats = [mm_output.pooler_output] if self.is_vilt else [mm_output.text_embeds, mm_output.image_embeds]\n","            \n","            if self.is_vilt and CFG.use_lstm:\n","                # First hidden state is apparently the embedding output\n","                # https://discuss.huggingface.co/t/hidden-states-embedding-tensors/3549/\n","                layerwise_cls = torch.stack([h[:, 0, :] for h in mm_output.hidden_states[1:]], dim=1)\n","                _, (h, _) = self.lstm(layerwise_cls)\n","                h = h.squeeze(dim=0)\n","                cats.append(h)\n","\n","            if self.is_vilt and CFG.use_mask_split:\n","                last_h = mm_output.last_hidden_state\n","                mask_len = mask.shape[1]\n","                mean_pooled_text = torch.mean(last_h[:, :mask_len, :], dim=1)\n","                mean_pooled_img = torch.mean(last_h[:, mask_len:, :], dim=1)\n","                cats += [mean_pooled_text, mean_pooled_img]\n","\n","            if self.is_vilt and CFG.use_attn:\n","                last_h = mm_output.last_hidden_state\n","                attentions = self.attn(last_h)\n","                x = torch.sum(attentions * last_h, dim=1)\n","\n","                cls = last_h[:, 0, :]\n","                x += cls\n","            elif self.is_vilt and CFG.use_modal_attn:\n","                last_h = mm_output.last_hidden_state\n","                mask_len = mask.shape[1]\n","                text_split = last_h[:, :mask_len, :]\n","                img_split = last_h[:, mask_len:, :]\n","                text_attentions = self.attn1(text_split)\n","                img_attentions = self.attn2(img_split)\n","                x1 = torch.sum(text_attentions * text_split, dim=1)\n","                x2 = torch.sum(img_attentions * img_split, dim=1)\n","\n","                x = torch.cat([x1, x2], dim=1)\n","\n","                cls = last_h[:, 0, :]\n","                cls = torch.cat([cls, cls], dim=1)\n","                x += cls\n","            else:\n","                x = torch.cat(cats, dim=1)\n","        elif self.use_dualencoder:\n","            de_output = self.de_model(input_ids=tokens, attention_mask=mask, pixel_values=image)\n","            x = torch.cat([de_output.text_embeds, de_output.image_embeds], dim=1)\n","        else:\n","            cls_text = self.text_model(tokens, attention_mask=mask).last_hidden_state[:, 0, :]\n","            cls_img = self.image_model(image).last_hidden_state[:, 0, :]\n","            x = torch.cat([cls_text, cls_img], dim=1)\n","\n","        x = self.fc1(x)\n","        x = self.activation(x)\n","        x = self.dropout(x)\n","        for hidden in self.hiddens:\n","            x = hidden(x)\n","            x = self.activation(x)\n","            x = self.dropout(x)\n","        x = self.fc2(x)\n","        \n","        output = x\n","\n","        if CFG.use_modal_attn:\n","            return output.float(), text_attentions, img_attentions\n","        \n","        return output.float()\n","    \n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            module.weight.data.normal_(mean=0.0, std=CFG.mlp_init_range)\n","            if module.bias is not None:\n","                module.bias.data.zero_()\n","        elif isinstance(module, nn.Embedding):\n","            module.weight.data.normal_(mean=0.0, std=CFG.mlp_init_range)\n","            if module.padding_idx is not None:\n","                module.weight.data[module.padding_idx].zero_()\n","        elif isinstance(module, nn.LayerNorm):\n","            module.bias.data.zero_()\n","            module.weight.data.fill_(1.0)"]},{"cell_type":"markdown","metadata":{},"source":["# Utility Functions"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T18:19:31.296299Z","iopub.status.busy":"2024-05-06T18:19:31.295514Z","iopub.status.idle":"2024-05-06T18:19:31.304096Z","shell.execute_reply":"2024-05-06T18:19:31.302978Z","shell.execute_reply.started":"2024-05-06T18:19:31.296263Z"},"trusted":true},"outputs":[],"source":["class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","\n","def asMinutes(s):\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return '%dm %ds' % (m, s)\n","\n","\n","def timeSince(since, percent):\n","    now = time.time()\n","    s = now - since\n","    es = s / (percent)\n","    rs = es - s\n","    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T18:19:31.726713Z","iopub.status.busy":"2024-05-06T18:19:31.726335Z","iopub.status.idle":"2024-05-06T18:19:31.731481Z","shell.execute_reply":"2024-05-06T18:19:31.730490Z","shell.execute_reply.started":"2024-05-06T18:19:31.726682Z"},"trusted":true},"outputs":[],"source":["def get_score(y_trues, y_preds):\n","    macro_f1 = f1_score(y_trues, y_preds, average='macro')\n","    return macro_f1"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-05-06T18:19:41.312411Z","iopub.status.busy":"2024-05-06T18:19:41.311584Z","iopub.status.idle":"2024-05-06T18:19:41.320936Z","shell.execute_reply":"2024-05-06T18:19:41.319856Z","shell.execute_reply.started":"2024-05-06T18:19:41.312379Z"},"trusted":true},"outputs":[],"source":["def test_loop(model, test_dataloader):\n","    all_soft = []\n","    all_hard = []\n","    all_ids = []\n","    all_tokens = []\n","    all_attentions = []\n","    \n","    model.eval()\n","    \n","    for identity, image, seq, mask in tqdm(test_dataloader):\n","        test_image = image.to(device=CFG.device)\n","        test_seq = seq.to(device=CFG.device)\n","        test_mask = mask.to(device=CFG.device)\n","\n","        truncated_seq = [s[m.bool()] for s, m in zip(test_seq, test_mask)]\n","\n","        with torch.no_grad():\n","            if CFG.use_modal_attn:\n","                output, text_attentions, img_attentions = model(test_seq, test_mask, test_image)\n","            else:\n","                output = model(test_seq, test_mask, test_image)\n","        \n","        soft = nn.functional.softmax(output, dim=1)\n","        hard = output.argmax(dim=1)\n","        \n","        all_ids += list(identity)\n","        all_soft.append(soft)\n","        all_hard.append(hard)\n","\n","        if CFG.use_modal_attn:\n","            for ts in truncated_seq:\n","                tokens = mm_processor.tokenizer.convert_ids_to_tokens(ts.cpu().numpy())\n","                all_tokens.append(tokens)\n","\n","            all_attentions.extend(text_attentions.squeeze().cpu().numpy())\n","        \n","    all_soft = torch.cat(all_soft, dim=0)\n","    all_hard = torch.cat(all_hard, dim=0)\n","\n","    if CFG.use_modal_attn:\n","        return all_ids, all_hard, all_soft, all_tokens, all_attentions\n","    \n","    return all_ids, all_hard, all_soft"]},{"cell_type":"markdown","metadata":{},"source":["# Inference From Checkpoint"]},{"cell_type":"code","execution_count":31,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["ConcatArch(\n","  (mm_model): ViltModel(\n","    (embeddings): ViltEmbeddings(\n","      (text_embeddings): TextEmbeddings(\n","        (word_embeddings): Embedding(30522, 768)\n","        (position_embeddings): Embedding(40, 768)\n","        (token_type_embeddings): Embedding(2, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (patch_embeddings): ViltPatchEmbeddings(\n","        (projection): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n","      )\n","      (token_type_embeddings): Embedding(2, 768)\n","      (dropout): Dropout(p=0.0, inplace=False)\n","    )\n","    (encoder): ViltEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x ViltLayer(\n","          (attention): ViltAttention(\n","            (attention): ViltSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","            (output): ViltSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","          )\n","          (intermediate): ViltIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ViltOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","      )\n","    )\n","    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","    (pooler): ViltPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (attn1): Sequential(\n","    (0): Linear(in_features=768, out_features=256, bias=True)\n","    (1): Tanh()\n","    (2): Linear(in_features=256, out_features=1, bias=True)\n","    (3): Softmax(dim=1)\n","  )\n","  (attn2): Sequential(\n","    (0): Linear(in_features=768, out_features=256, bias=True)\n","    (1): Tanh()\n","    (2): Linear(in_features=256, out_features=1, bias=True)\n","    (3): Softmax(dim=1)\n","  )\n","  (fc1): Linear(in_features=1536, out_features=256, bias=True)\n","  (hiddens): ModuleList()\n","  (fc2): Linear(in_features=256, out_features=2, bias=True)\n","  (activation): ReLU()\n","  (dropout): Dropout(p=0.1, inplace=False)\n",")"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["collate = Collator(test=True)\n","valid_dataloader = DataLoader(val_dataset, batch_size=CFG.batch_size, collate_fn=collate)\n","loss_fn = nn.CrossEntropyLoss()\n","\n","inf_model_name = 'dandelin-vilt-b32-mlm-mattn_score_0.9115'\n","inf_model = ConcatArch(\n","    hidden_size=CFG.mlp_hidden_size,\n","    hidden_layers=CFG.mlp_hidden_layers,\n","    dropout=CFG.mlp_dropout,\n","    num_classes=CFG.num_class,\n","    use_multimodal=CFG.use_multimodal,\n","    use_dualencoder=CFG.use_dualencoder,\n","    is_mclip=CFG.is_mclip\n",").to(CFG.device)\n","inf_model.load_state_dict(torch.load('model_backup/T4/' + inf_model_name + '.pth', map_location=torch.device(CFG.device))['model'])\n","inf_model"]},{"cell_type":"code","execution_count":33,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.18s/it]\n"]}],"source":["ids, hards, softs, tokens, attentions = test_loop(inf_model, valid_dataloader)"]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[],"source":["def plot_attention_heatmap(tokens, attentions, idx):\n","    a = list(attentions[1:len(tokens) - 1])\n","    t = tokens[1:-1]\n","\n","    print(a)\n","    print(t)\n","    \n","    plt.figure(figsize=(10, 8))\n","    sns.heatmap(a, annot=True, cmap='viridis', xticklabels=t, yticklabels=tokens)\n","    plt.title(\"Attention Heatmap\")\n","    plt.ylabel('Input Tokens')\n","    plt.xlabel('Output Tokens')\n","    plt.show()"]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[0.20530574, 0.03372386, 0.017652327, 0.029201869, 0.006606217, 0.015603379, 0.062646054, 0.27612308, 0.0023764307, 0.0015391929, 0.018184192, 0.20514084, 0.0031150633, 0.020074347, 0.07724695]\n","['el', 'fis', '##ico', 'at', '##rae', 'per', '##o', 'la', 'en', '##fer', '##mer', '##a', 'en', '##amo', '##ra']\n"]},{"ename":"IndexError","evalue":"Inconsistent shape between the condition and the input (got (15, 1) and (15,))","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[1;32mIn[63], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mplot_attention_heatmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattentions\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n","Cell \u001b[1;32mIn[62], line 9\u001b[0m, in \u001b[0;36mplot_attention_heatmap\u001b[1;34m(tokens, attentions, idx)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(t)\n\u001b[0;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n\u001b[1;32m----> 9\u001b[0m \u001b[43msns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheatmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mviridis\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxticklabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myticklabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttention Heatmap\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput Tokens\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","File \u001b[1;32mc:\\Users\\LaptopAid\\miniconda3\\Lib\\site-packages\\seaborn\\matrix.py:446\u001b[0m, in \u001b[0;36mheatmap\u001b[1;34m(data, vmin, vmax, cmap, center, robust, annot, fmt, annot_kws, linewidths, linecolor, cbar, cbar_kws, cbar_ax, square, xticklabels, yticklabels, mask, ax, **kwargs)\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Plot rectangular data as a color-encoded matrix.\u001b[39;00m\n\u001b[0;32m    366\u001b[0m \n\u001b[0;32m    367\u001b[0m \u001b[38;5;124;03mThis is an Axes-level function and will draw the heatmap into the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    443\u001b[0m \n\u001b[0;32m    444\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;66;03m# Initialize the plotter object\u001b[39;00m\n\u001b[1;32m--> 446\u001b[0m plotter \u001b[38;5;241m=\u001b[39m \u001b[43m_HeatMapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvmin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvmax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcenter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrobust\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mannot_kws\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbar_kws\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxticklabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    448\u001b[0m \u001b[43m                      \u001b[49m\u001b[43myticklabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;66;03m# Add the pcolormesh kwargs here\u001b[39;00m\n\u001b[0;32m    451\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinewidths\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m linewidths\n","File \u001b[1;32mc:\\Users\\LaptopAid\\miniconda3\\Lib\\site-packages\\seaborn\\matrix.py:115\u001b[0m, in \u001b[0;36m_HeatMapper.__init__\u001b[1;34m(self, data, vmin, vmax, cmap, center, robust, annot, fmt, annot_kws, cbar, cbar_kws, xticklabels, yticklabels, mask)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m# Validate the mask and convert to DataFrame\u001b[39;00m\n\u001b[0;32m    113\u001b[0m mask \u001b[38;5;241m=\u001b[39m _matrix_mask(data, mask)\n\u001b[1;32m--> 115\u001b[0m plot_data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_where\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# Get good names for the rows and columns\u001b[39;00m\n\u001b[0;32m    118\u001b[0m xtickevery \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n","File \u001b[1;32mc:\\Users\\LaptopAid\\miniconda3\\Lib\\site-packages\\numpy\\ma\\core.py:1933\u001b[0m, in \u001b[0;36mmasked_where\u001b[1;34m(condition, a, copy)\u001b[0m\n\u001b[0;32m   1931\u001b[0m (cshape, ashape) \u001b[38;5;241m=\u001b[39m (cond\u001b[38;5;241m.\u001b[39mshape, a\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m   1932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cshape \u001b[38;5;129;01mand\u001b[39;00m cshape \u001b[38;5;241m!=\u001b[39m ashape:\n\u001b[1;32m-> 1933\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInconsistent shape between the condition and the input\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1934\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (cshape, ashape))\n\u001b[0;32m   1935\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_mask\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m   1936\u001b[0m     cond \u001b[38;5;241m=\u001b[39m mask_or(cond, a\u001b[38;5;241m.\u001b[39m_mask)\n","\u001b[1;31mIndexError\u001b[0m: Inconsistent shape between the condition and the input (got (15, 1) and (15,))"]},{"data":{"text/plain":["<Figure size 1000x800 with 0 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["plot_attention_heatmap(tokens[0], attentions[0], 0)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4894786,"sourceId":8249731,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":4}
